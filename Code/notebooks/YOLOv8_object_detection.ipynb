{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a84c5a60-ae3c-4048-b845-7be6ee10191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "YOLOv8-based real-time object detection on video streams.\n",
    "Author: Maryam Khalid\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import random\n",
    "from ultralytics import YOLO\n",
    "import opencv_jupyter_ui as jcv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08e3a17e-8ffc-4356-83b2-0156f613fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Yolov8 model\n",
    "yolo = YOLO(\"yolov8s.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b756c38a-badd-42cd-8051-d9dc9c6b4759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getColours(cls_num):\n",
    "    \"\"\"Generate unique colors for each class ID\"\"\"\n",
    "    random.seed(cls_num)\n",
    "    return tuple(random.randint(0, 255) for _ in range(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "087703a7-3334-447b-8aaa-54621bc0e619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input Video Path\n",
    "video_path = \"sample.mp4\"\n",
    "videoCap = cv2.VideoCapture(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8758abc8-ea56-4f4e-9e1c-b10de11466fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 16 cars, 157.9ms\n",
      "Speed: 3.2ms preprocess, 157.9ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fdef9bd27984b5ba013d22b2ec08adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='danger', description='Stop', style=ButtonStyle()), HBox(children=(Label(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26659e895fc5481fbf3bb0f62c77ab84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center>True</center>'), Canvas()), layout=Layout(border_bottom='1.5px solid', bord…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 15 cars, 218.3ms\n",
      "Speed: 4.8ms preprocess, 218.3ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 16 cars, 1 bus, 151.2ms\n",
      "Speed: 4.6ms preprocess, 151.2ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 1 bus, 149.8ms\n",
      "Speed: 1.9ms preprocess, 149.8ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 151.2ms\n",
      "Speed: 2.3ms preprocess, 151.2ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 1 bus, 151.7ms\n",
      "Speed: 2.8ms preprocess, 151.7ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 144.1ms\n",
      "Speed: 3.3ms preprocess, 144.1ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 147.9ms\n",
      "Speed: 2.1ms preprocess, 147.9ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 cars, 153.5ms\n",
      "Speed: 2.9ms preprocess, 153.5ms inference, 6.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 cars, 1 bus, 161.6ms\n",
      "Speed: 2.6ms preprocess, 161.6ms inference, 6.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 158.7ms\n",
      "Speed: 2.1ms preprocess, 158.7ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 178.4ms\n",
      "Speed: 1.9ms preprocess, 178.4ms inference, 7.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 15 cars, 162.9ms\n",
      "Speed: 3.4ms preprocess, 162.9ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 1 bus, 146.3ms\n",
      "Speed: 2.0ms preprocess, 146.3ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 cars, 146.4ms\n",
      "Speed: 1.9ms preprocess, 146.4ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 153.4ms\n",
      "Speed: 1.7ms preprocess, 153.4ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 190.3ms\n",
      "Speed: 1.9ms preprocess, 190.3ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 cars, 150.7ms\n",
      "Speed: 4.1ms preprocess, 150.7ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 208.4ms\n",
      "Speed: 40.3ms preprocess, 208.4ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 cars, 147.2ms\n",
      "Speed: 1.8ms preprocess, 147.2ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 cars, 153.1ms\n",
      "Speed: 1.7ms preprocess, 153.1ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = videoCap.read()   #videoCap.read(): Reads one frame from video.\n",
    "    if not ret:\n",
    "        break\n",
    "    results = yolo.track(frame, stream=True)     #yolo.track(frame, stream=True): Runs YOLOv8 object detection.\n",
    "\n",
    "    for result in results:\n",
    "        class_names = result.names\n",
    "        for box in result.boxes:           #result.boxes: Contains bounding boxes for detected objects.\n",
    "            if box.conf[0] > 0.4:                #box.conf[0]:Confidence score of detection.\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])  #box.xyxy[0]: Coordinates of bounding box (x1,y1,x2,y2).\n",
    "\n",
    "                cls = int(box.cls[0])\n",
    "                class_name = class_names[cls]\n",
    "\n",
    "                conf = float(box.conf[0])           \n",
    "\n",
    "                colour = getColours(cls)\n",
    "\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), colour, 2)     #cv2.rectangle(): Draws bounding box.\n",
    "\n",
    "                cv2.putText(frame, f\"{class_name} {conf:.2f}\",           #cv2.putText(): Adds class name + confidence on frame.\n",
    "                            (x1, max(y1 - 10, 20)), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            0.6, colour, 2)\n",
    "\n",
    "    if frame_count < 20:\n",
    "        jcv2.imshow(ret, frame)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "videoCap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc619ab-f937-49ab-a983-1a57774aaf18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43602472-7cbf-4bf8-a739-31c6614b1a78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
